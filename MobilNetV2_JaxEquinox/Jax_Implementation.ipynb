{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax Implementation of MobileNetV2\n",
    "The following is a jax and equinox implementation of the MobileNetV2 architecture\n",
    "\n",
    "The implementation is broken up into its individual (modulo stride) layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mImportError: /home/chris/anaconda3/envs/epitaxy/lib/python3.12/lib-dynload/_sqlite3.cpython-312-x86_64-linux-gnu.so: undefined symbol: sqlite3_deserialize. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from typing import List, Any, Tuple, Optional, Union, Sequence\n",
    "from jaxtyping import Array, Float, Int, PyTree\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "import equinox as eqx\n",
    "\n",
    "import optax as opt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2\n",
    "\n",
    "The following is the full implementation of the MobileNetV2 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Depthwise Separable Convolution Layer\n",
    "class DepthwiseSeparableConv(eqx.Module):\n",
    "    depthwise: eqx.nn.Conv2d\n",
    "    pointwise: eqx.nn.Conv2d\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, key):\n",
    "        dw_key, pw_key = jax.random.split(key)\n",
    "        self.depthwise = eqx.nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=in_channels, \n",
    "            kernel_size=(3, 3), \n",
    "            stride=stride, \n",
    "            padding=1, \n",
    "            groups=in_channels,\n",
    "            key=dw_key\n",
    "        )\n",
    "        self.pointwise = eqx.nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels, \n",
    "            kernel_size=(3, 3), \n",
    "            key=pw_key\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [reference](https://github.com/DarshanDeshpande/jax-models/blob/main/jax_models/layers/depthwise_separable_conv.py)\n",
    "class DepthwiseConv2D(eqx.Module):\n",
    "    in_channels: int = eqx.field(static=True)\n",
    "    out_channels: int = eqx.field(static=True)\n",
    "    kernel_size: Union[int, Sequence[int]] = eqx.field(static=True)\n",
    "    stride: Union[int, Sequence[int]] = eqx.field(static=True)\n",
    "    padding: Union[int, Sequence[int]] = eqx.field(static=True)\n",
    "    depth_multiplier: int = eqx.field(static=True)\n",
    "    use_bias: bool = eqx.field(static=True)\n",
    "    groups: int = eqx.field(static=True)\n",
    "    key: Any = eqx.field(static=True)\n",
    "\n",
    "    kernel: Array\n",
    "    bias: Array\n",
    "\n",
    "    def __init__(self, in_channels: int, depth_multiplier: int, kernel_size: Tuple[int, int], stride: Tuple[int, int], padding: Tuple[int, int], use_bias: bool, key: jr.PRNGKey):\n",
    "        self.in_channels = in_channels\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "        self.out_channels = in_channels * depth_multiplier\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.use_bias = False\n",
    "        self.groups = in_channels\n",
    "        self.key = key\n",
    "\n",
    "        self.kernel = jr.normal(\n",
    "            key,\n",
    "            shape=(self.depth_multiplier * self.in_channels, 1) + self.kernel_size)\n",
    "\n",
    "        if use_bias:\n",
    "            self.bias = jr.normal(key, shape=(in_channels * depth_multiplier,))\n",
    "            self.use_bias = True\n",
    "        else:\n",
    "            self.bias = jnp.zeros((in_channels * depth_multiplier,))\n",
    "\n",
    "\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        x = x.reshape((x.shape[0], 1, *x.shape[1:]))\n",
    "        x = jax.lax.conv_general_dilated(\n",
    "            lhs=x,\n",
    "            rhs=self.kernel,\n",
    "            window_strides=self.stride,\n",
    "            padding='VALID',\n",
    "            lhs_dilation=(1,) * len(self.kernel_size),\n",
    "            rhs_dilation=(1,) * len(self.kernel_size),\n",
    "            dimension_numbers=(\"NCHW\", \"OIHW\", \"NCHW\"),\n",
    "            # feature_group_count=x.shape[-1]\n",
    "            feature_group_count=1\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an inverted residual block [reference](https://github.com/keras-team/keras/blob/v3.3.3/keras/src/applications/mobilenet_v2.py#L398)\n",
    "class InvertedResidualBlock(eqx.Module):\n",
    "    # static fields get ignored durign training\n",
    "    in_channels:  int   = eqx.field(static=True)\n",
    "    expansion:    int   = eqx.field(static=True)\n",
    "    stride:       int   = eqx.field(static=True)\n",
    "    alpha:        float = eqx.field(static=True)\n",
    "    filters:      int   = eqx.field(static=True)\n",
    "    pw_filters:   int   = eqx.field(static=True)\n",
    "    block_id:     int   = eqx.field(static=True)\n",
    "\n",
    "    layers: List[Any]\n",
    "    \n",
    "\n",
    "    def _make_divisible(self, v, divisor, min_value=None):\n",
    "        if min_value is None:\n",
    "            min_value = divisor\n",
    "        new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "        # Make sure that round down does not go down by more than 10%.\n",
    "        if new_v < 0.9 * v:\n",
    "            new_v += divisor\n",
    "        return new_v\n",
    "\n",
    "    def __init__(self, in_channels: int, expansion: int, stride: int, alpha: float, filters: int, block_id: int, key: jr.PRNGKey, BATCH_SIZE: int):\n",
    "        self.in_channels = in_channels\n",
    "        self.expansion = expansion\n",
    "        self.stride = stride\n",
    "        self.alpha = alpha\n",
    "        self.filters = filters\n",
    "        self.block_id = block_id\n",
    "\n",
    "        pointwise_filters = int(filters * alpha)\n",
    "        # ensure that the number of filters on the last 1x1 convolution is a multiple of 8\n",
    "        pointwise_filters = self._make_divisible(pointwise_filters, 8)\n",
    "        self.pw_filters = pointwise_filters\n",
    "\n",
    "        # Define the key for the block\n",
    "        key, conv_key = jr.split(key)\n",
    "        self.layers = []\n",
    "\n",
    "        # Define the layers of the block\n",
    "        if block_id:\n",
    "            # Expand with a pointwise 1x1 convolution\n",
    "            self.layers.extend([\n",
    "                eqx.nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels * expansion,\n",
    "                    kernel_size=(1, 1),\n",
    "                    use_bias=False,\n",
    "                    key=conv_key\n",
    "                ),\n",
    "                eqx.nn.BatchNorm(\n",
    "                    BATCH_SIZE,\n",
    "                    axis_name='batch',\n",
    "                    eps=1e-3,\n",
    "                    momentum=0.99\n",
    "                ),\n",
    "                jax.nn.relu6\n",
    "            ])\n",
    "        \n",
    "        self.layers.extend([\n",
    "            DepthwiseConv2D(\n",
    "                in_channels=in_channels * expansion,\n",
    "                depth_multiplier=1,\n",
    "                kernel_size=(3, 3),\n",
    "                stride=(stride, stride),\n",
    "                padding=(1, 1),\n",
    "                key=conv_key,\n",
    "                use_bias=False\n",
    "            ),\n",
    "            eqx.nn.BatchNorm(\n",
    "                BATCH_SIZE,\n",
    "                axis_name='batch',\n",
    "                eps=1e-3,\n",
    "                momentum=0.99\n",
    "            ),\n",
    "            jax.nn.relu6\n",
    "        ])\n",
    "\n",
    "        # pointwise 1x1 conv\n",
    "        self.layers.extend([\n",
    "            eqx.nn.Conv2d(\n",
    "                in_channels=in_channels * expansion,\n",
    "                out_channels=pointwise_filters,\n",
    "                kernel_size=(1, 1),\n",
    "                use_bias=False,\n",
    "                key=conv_key\n",
    "            ),\n",
    "            eqx.nn.BatchNorm(\n",
    "                BATCH_SIZE,\n",
    "                axis_name='batch',\n",
    "                eps=1e-3,\n",
    "                momentum=0.99\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x, state):\n",
    "        input = x\n",
    "\n",
    "        lc = 0\n",
    "        \n",
    "        if self.block_id:\n",
    "            x = self.layers[0](x)\n",
    "            x, state = self.layers[1](x, state)\n",
    "            x = self.layers[2](x)\n",
    "            lc = 3\n",
    "        if self.stride == 2:\n",
    "            x = jnp.pad(x, 1, mode='constant', constant_values=0)\n",
    "\n",
    "        for _, layer in enumerate(self.layers[lc:]):\n",
    "            if issubclass(type(layer), eqx.nn.StatefulLayer):\n",
    "                x, state = layer(x, state)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        if self.in_channels == self.pw_filters and self.stride == 1:\n",
    "            x = x + input\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Bottleneck Block\n",
    "class Bottleneck(eqx.Module):\n",
    "    _stride: int = eqx.field(static=True)\n",
    "\n",
    "    conv1: eqx.nn.Conv2d\n",
    "    depthwise_conv: DepthwiseSeparableConv\n",
    "    conv3: eqx.nn.Conv2d\n",
    "    use_residual: bool\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio, use_residual, key: jr.PRNGKey):\n",
    "        self._stride=stride\n",
    "        keys = jr.split(key, 3)\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.conv1 = eqx.nn.Conv2d(in_channels, hidden_dim, kernel_size=(1, 1), key=keys[0])\n",
    "        self.depthwise_conv = [DepthwiseSeparableConv(hidden_dim, hidden_dim, stride=1, key=keys[1]),\n",
    "                               DepthwiseSeparableConv(hidden_dim, hidden_dim, stride=2, key=keys[1])]\n",
    "        self.conv3 = eqx.nn.Conv2d(hidden_dim, out_channels, kernel_size=(1, 1), key=keys[2])\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        if self._stride == 1:\n",
    "            x = self.depthwise_conv[0](x)\n",
    "            x = jax.nn.relu(x)\n",
    "            x = self.conv3(x)\n",
    "            return x + residual\n",
    "        else:\n",
    "            x = self.depthwise_conv[1](x)\n",
    "            x = jax.nn.relu(x)\n",
    "            x = self.conv3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MobileNetV2\n",
    "class MobileNetV2(eqx.Module):\n",
    "    in_channels: int = eqx.field(static=True)\n",
    "    \n",
    "    first_conv: eqx.nn.Conv2d\n",
    "    bottlenecks: list\n",
    "    last_conv: eqx.nn.Conv2d\n",
    "    pool: eqx.nn.AvgPool2d\n",
    "    classifier: eqx.nn.Conv2d\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, key):\n",
    "        keys = jax.random.split(key, 10)\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.first_conv = eqx.nn.Conv2d(in_channels, 32, kernel_size=(3, 3), stride=2, padding=1, key=keys[0])\n",
    "\n",
    "        # Bottleneck blocks configuration\n",
    "        bottleneck_configs = [\n",
    "            # (in_channels, out_channels, stride, expand_ratio, n_repeats)\n",
    "            (32, 16, 1, 1, 1),   # First block, no expansion, no repetition\n",
    "            (16, 24, 2, 6, 2),   # Second block, 2x stride, 2 repetitions\n",
    "            (24, 32, 2, 6, 3),   # Third block, 2x stride, 3 repetitions\n",
    "            (32, 64, 2, 6, 4),   # Fourth block, 2x stride, 4 repetitions\n",
    "            (64, 96, 1, 6, 3),   # Fifth block, stride 1, 3 repetitions\n",
    "            (96, 160, 2, 6, 3),  # Sixth block, 2x stride, 3 repetitions\n",
    "            (160, 320, 1, 6, 1), # Seventh block, stride 1, no repetition\n",
    "        ]\n",
    "\n",
    "        self.bottlenecks = []\n",
    "        current_key = keys[1]\n",
    "\n",
    "        for config in bottleneck_configs:\n",
    "            in_channels, out_channels, stride, expand_ratio, n_repeats = config\n",
    "\n",
    "            # Add the first block in the stage with the specified stride\n",
    "            self.bottlenecks.append(\n",
    "                Bottleneck(in_channels, out_channels, stride, expand_ratio, use_residual=(stride == 1), key=current_key)\n",
    "            )\n",
    "            current_key = jax.random.split(current_key, 1)[0]\n",
    "\n",
    "            # Add the remaining blocks with stride = 1\n",
    "            for i in range(n_repeats - 1):\n",
    "                self.bottlenecks.append(\n",
    "                    Bottleneck(out_channels, out_channels, stride=1, expand_ratio=expand_ratio, use_residual=True, key=current_key)\n",
    "                )\n",
    "                current_key = jax.random.split(current_key, 1)[0]\n",
    "\n",
    "        self.last_conv = eqx.nn.Conv2d(24, 1280, kernel_size=(1, 1), key=keys[2])\n",
    "        self.pool = eqx.nn.AvgPool2d(kernel_size=(7, 7))\n",
    "        self.classifier = eqx.nn.Conv2d(1280, num_classes, kernel_size=(1,1),key=keys[3])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        x = jax.nn.relu(x)\n",
    "\n",
    "        for bottleneck in self.bottlenecks:\n",
    "            x = bottleneck(x)\n",
    "\n",
    "        x = self.last_conv(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = jnp.mean(x, axis=(1, 2))  # Global average pooling\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2 model based on the Keras implementation\n",
    "class MobileNetV2_K(eqx.Module):\n",
    "    layers: List[Any]\n",
    "\n",
    "    def _make_divisible(self, v, divisor, min_value=None):\n",
    "        if min_value is None:\n",
    "            min_value = divisor\n",
    "        new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "        # Make sure that round down does not go down by more than 10%.\n",
    "        if new_v < 0.9 * v:\n",
    "            new_v += divisor\n",
    "        return new_v\n",
    "\n",
    "    def __init__(self,\n",
    "        in_channels: int = 3,\n",
    "        alpha: float = 1.0,\n",
    "        include_top: bool = True,\n",
    "        num_classes: int = 1000,\n",
    "        classifier_activation: str = 'softmax',\n",
    "        pooling: Optional[str] = None,\n",
    "        key: jr.PRNGKey = jr.PRNGKey(0),\n",
    "        BATCH_SIZE: int = 32\n",
    "    ):\n",
    "        key, conv_key = jr.split(key)\n",
    "        \n",
    "        first_block_filters = self._make_divisible(32 * alpha, 8)\n",
    "        if alpha > 1.0:\n",
    "            last_block_filters = self._make_divisible(1280 * alpha, 8)\n",
    "        else:\n",
    "            last_block_filters = 1280\n",
    "\n",
    "        self.layers = [\n",
    "            eqx.nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=first_block_filters,\n",
    "                kernel_size=(3, 3),\n",
    "                stride=(2, 2),\n",
    "                use_bias=False,\n",
    "                key=conv_key\n",
    "            ),\n",
    "            eqx.nn.BatchNorm(\n",
    "                input_size=first_block_filters,\n",
    "                eps=1e-3,\n",
    "                momentum=0.99,\n",
    "                axis_name=\"batch\"\n",
    "            ),\n",
    "            jax.nn.relu6,\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=first_block_filters,\n",
    "                expansion=1,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=16,\n",
    "                block_id=0,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=16,\n",
    "                expansion=6,\n",
    "                stride=2,\n",
    "                alpha=alpha,\n",
    "                filters=24,\n",
    "                block_id=1,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=24,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=24,\n",
    "                block_id=2,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=24,\n",
    "                expansion=6,\n",
    "                stride=2,\n",
    "                alpha=alpha,\n",
    "                filters=32,\n",
    "                block_id=3,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=32,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=32,\n",
    "                block_id=4,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=32,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=32,\n",
    "                block_id=5,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=32,\n",
    "                expansion=6,\n",
    "                stride=2,\n",
    "                alpha=alpha,\n",
    "                filters=64,\n",
    "                block_id=6,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=64,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=64,\n",
    "                block_id=7,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=64,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=64,\n",
    "                block_id=8,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=64,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=64,\n",
    "                block_id=9,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=64,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=96,\n",
    "                block_id=10,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=96,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=96,\n",
    "                block_id=11,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=96,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=96,\n",
    "                block_id=12,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=96,\n",
    "                expansion=6,\n",
    "                stride=2,\n",
    "                alpha=alpha,\n",
    "                filters=160,\n",
    "                block_id=13,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=160,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=160,\n",
    "                block_id=14,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=160,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=160,\n",
    "                block_id=15,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            InvertedResidualBlock(\n",
    "                in_channels=160,\n",
    "                expansion=6,\n",
    "                stride=1,\n",
    "                alpha=alpha,\n",
    "                filters=320,\n",
    "                block_id=16,\n",
    "                key=key,\n",
    "                BATCH_SIZE=BATCH_SIZE\n",
    "            ),\n",
    "            eqx.nn.Conv2d(\n",
    "                in_channels=320,\n",
    "                out_channels=last_block_filters,\n",
    "                kernel_size=(1, 1),\n",
    "                use_bias=False,\n",
    "                key=conv_key\n",
    "            ),\n",
    "            eqx.nn.BatchNorm(\n",
    "                input_size=BATCH_SIZE,\n",
    "                eps=1e-3,\n",
    "                momentum=0.999,\n",
    "                axis_name=\"batch\"\n",
    "            ),\n",
    "            jax.nn.relu6\n",
    "        ]\n",
    "\n",
    "        if include_top:\n",
    "            self.layers.extend([\n",
    "                eqx.nn.AvgPool2d(kernel_size=(7, 7)), # TODO: replace with global average pooling\n",
    "                eqx.nn.Linear(\n",
    "                    in_features=last_block_filters,\n",
    "                    out_features=num_classes,\n",
    "                    key=key\n",
    "                )\n",
    "            ])\n",
    "            if classifier_activation == 'softmax':\n",
    "                self.layers.append(jax.nn.softmax)\n",
    "\n",
    "        else:\n",
    "            if pooling == 'avg':\n",
    "                self.layers.append(eqx.nn.AvgPool2d(kernel_size=(7, 7))) # TODO: replace with global average pooling\n",
    "            elif pooling == 'max':\n",
    "                self.layers.append(eqx.nn.MaxPool2d(kernel_size=(7, 7))) # TODO: replace with global max pooling\n",
    "\n",
    "    def __call__(self, x, state):\n",
    "        for layer in self.layers:\n",
    "            if issubclass(type(layer), eqx.nn.StatefulLayer) or isinstance(layer, InvertedResidualBlock):\n",
    "                x, state = layer(x, state)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 300\n",
    "BATCH_SIZE = 32\n",
    "PRINT_EVERY = 30\n",
    "SEED = 42\n",
    "\n",
    "# Key generation\n",
    "key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test with MNIST\n",
    "\n",
    "# Load the MNIST dataset [reference](https://docs.kidger.site/equinox/examples/mnist/#the-dataset)\n",
    "normalise_data = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# we aren't using a validation set here, but that's easy enough to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trainloader))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # 64x1x28x28\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, state = eqx.nn.make_with_state(MobileNetV2_K)(in_channels=1, num_classes=10, key=key, BATCH_SIZE=BATCH_SIZE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (32,), (64,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, state, opt_state\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Example loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_value\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# scalar loss\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Example inference\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(model, state, x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\n\u001b[1;32m      4\u001b[0m     model: MobileNetV2_K,  state: eqx\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mState, x: Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch 1 28 28\u001b[39m\u001b[38;5;124m\"\u001b[39m], y: Int[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m batch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      6\u001b[0m     batch_model \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(\n\u001b[1;32m      7\u001b[0m         model, axis_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), out_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0m     pred_y, state \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy(y, pred_y)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[17], line 255\u001b[0m, in \u001b[0;36mMobileNetV2_K.__call__\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(layer), eqx\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mStatefulLayer) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, InvertedResidualBlock):\n\u001b[0;32m--> 255\u001b[0m         x, state \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "Cell \u001b[0;32mIn[14], line 112\u001b[0m, in \u001b[0;36mInvertedResidualBlock.__call__\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[lc:]):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(layer), eqx\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mStatefulLayer):\n\u001b[0;32m--> 112\u001b[0m         x, state \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/epitaxy/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/epitaxy/lib/python3.12/site-packages/equinox/nn/_batch_norm.py:162\u001b[0m, in \u001b[0;36mBatchNorm.__call__\u001b[0;34m(self, x, state, key, inference)\u001b[0m\n\u001b[1;32m    160\u001b[0m running_mean, running_var \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_index)\n\u001b[1;32m    161\u001b[0m momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum\n\u001b[0;32m--> 162\u001b[0m running_mean \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_mean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    163\u001b[0m running_var \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m momentum) \u001b[38;5;241m*\u001b[39m batch_var \u001b[38;5;241m+\u001b[39m momentum \u001b[38;5;241m*\u001b[39m running_var\n\u001b[1;32m    164\u001b[0m running_mean \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mselect(first_time, batch_mean, running_mean)\n",
      "File \u001b[0;32m~/anaconda3/envs/epitaxy/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:272\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    270\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 272\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/epitaxy/lib/python3.12/site-packages/jax/_src/numpy/ufuncs.py:206\u001b[0m, in \u001b[0;36madd\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@implements\u001b[39m(np\u001b[38;5;241m.\u001b[39madd, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, inline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(x: ArrayLike, y: ArrayLike, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    205\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_or(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/epitaxy/lib/python3.12/site-packages/jax/_src/lax/lax.py:1725\u001b[0m, in \u001b[0;36mbroadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   1723\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1725\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1726\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: add got incompatible shapes for broadcasting: (32,), (64,)."
     ]
    }
   ],
   "source": [
    "# for MobileNetV2_K\n",
    "\n",
    "def loss(\n",
    "    model: MobileNetV2_K,  state: eqx.nn.State, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    batch_model = jax.vmap(\n",
    "        model, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n",
    "    )\n",
    "    pred_y, state = batch_model(x, state)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # y are the true targets, and should be integers 0-9.\n",
    "    # pred_y are the log-softmax'd predictions.\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, state, opt_state, xs, ys):\n",
    "    grads, state = eqx.filter_grad(loss, has_aux=True)(model, state, xs, ys)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, state, opt_state\n",
    "\n",
    "# Example loss\n",
    "loss_value = loss(model, state, dummy_x, dummy_y)\n",
    "print(loss_value.shape)  # scalar loss\n",
    "# Example inference\n",
    "output = jax.vmap(model)(dummy_x, state)\n",
    "print(output.shape)  # batch of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(in_channels=1, num_classes=10, key=key)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MobileNetV2\n",
    "\n",
    "def loss(\n",
    "    model: MobileNetV2, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # Our input has the shape (BATCH_SIZE, 1, 28, 28), but our model operations on\n",
    "    # a single input input image of shape (1, 28, 28).\n",
    "    #\n",
    "    # Therefore, we have to use jax.vmap, which in this case maps our model over the\n",
    "    # leading (batch) axis.\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # y are the true targets, and should be integers 0-9.\n",
    "    # pred_y are the log-softmax'd predictions.\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "\n",
    "# Example loss\n",
    "loss_value = loss(model, dummy_x, dummy_y)\n",
    "print(loss_value.shape)  # scalar loss\n",
    "# Example inference\n",
    "output = jax.vmap(model)(dummy_x)\n",
    "print(output.shape)  # batch of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
